
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/demo_lfsi.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_demo_lfsi.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_demo_lfsi.py:


Demo Filtered Spectral Initialization
===================================================

.. GENERATED FROM PYTHON SOURCE LINES 8-10

Select Working Directory and Device
-----------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 10-38

.. code-block:: Python

    import os

    from torch.utils import data

    os.chdir(os.path.dirname(os.getcwd()))
    print("Current Working Directory ", os.getcwd())

    import sys

    sys.path.append(os.path.join(os.getcwd()))

    # General imports
    import matplotlib.pyplot as plt
    import torch
    import os

    # Set random seed for reproducibility
    torch.manual_seed(0)

    manual_device = "cpu"
    # Check GPU support
    print("GPU support: ", torch.cuda.is_available())

    if manual_device:
        device = manual_device
    else:
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Current Working Directory  /home/runner/work/pycolibri/pycolibri
    GPU support:  False




.. GENERATED FROM PYTHON SOURCE LINES 39-41

Load dataset
-----------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 41-51

.. code-block:: Python

    from colibri.data.datasets import CustomDataset
    import torchvision
    name = "cifar10"
    path = "."
    batch_size = 1


    dataset = CustomDataset(name, path)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0.00/170M [00:00<?, ?B/s]      0%|          | 32.8k/170M [00:00<11:04, 257kB/s]      0%|          | 229k/170M [00:00<02:49, 1.00MB/s]      1%|          | 885k/170M [00:00<00:54, 3.11MB/s]      2%|▏         | 3.44M/170M [00:00<00:15, 11.1MB/s]      4%|▍         | 6.82M/170M [00:00<00:09, 17.6MB/s]      8%|▊         | 13.4M/170M [00:00<00:05, 29.1MB/s]     12%|█▏        | 19.7M/170M [00:00<00:04, 35.3MB/s]     15%|█▌        | 26.1M/170M [00:00<00:03, 39.1MB/s]     19%|█▉        | 32.4M/170M [00:01<00:03, 41.6MB/s]     23%|██▎       | 38.8M/170M [00:01<00:03, 43.1MB/s]     27%|██▋       | 45.5M/170M [00:01<00:02, 44.6MB/s]     30%|███       | 51.9M/170M [00:01<00:02, 45.3MB/s]     34%|███▍      | 58.2M/170M [00:01<00:02, 45.7MB/s]     38%|███▊      | 64.5M/170M [00:01<00:02, 46.0MB/s]     42%|████▏     | 71.0M/170M [00:01<00:02, 46.4MB/s]     45%|████▌     | 77.3M/170M [00:02<00:02, 46.4MB/s]     49%|████▉     | 84.0M/170M [00:02<00:01, 46.9MB/s]     53%|█████▎    | 90.3M/170M [00:02<00:01, 46.9MB/s]     57%|█████▋    | 96.7M/170M [00:02<00:01, 46.8MB/s]     60%|██████    | 103M/170M [00:02<00:01, 46.7MB/s]      64%|██████▍   | 109M/170M [00:02<00:01, 46.9MB/s]     68%|██████▊   | 116M/170M [00:02<00:01, 46.7MB/s]     72%|███████▏  | 122M/170M [00:03<00:01, 46.7MB/s]     75%|███████▌  | 128M/170M [00:03<00:00, 46.7MB/s]     79%|███████▉  | 135M/170M [00:03<00:00, 46.8MB/s]     83%|████████▎ | 141M/170M [00:03<00:00, 46.9MB/s]     87%|████████▋ | 148M/170M [00:03<00:00, 46.9MB/s]     90%|█████████ | 154M/170M [00:03<00:00, 46.9MB/s]     94%|█████████▍| 161M/170M [00:03<00:00, 46.8MB/s]     98%|█████████▊| 167M/170M [00:03<00:00, 47.0MB/s]    100%|██████████| 170M/170M [00:04<00:00, 41.9MB/s]




.. GENERATED FROM PYTHON SOURCE LINES 52-54

Visualize dataset
-----------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 54-63

.. code-block:: Python

    from torchvision.utils import make_grid

    sample = dataset[0]["input"]
    sample = sample.mean(0)
    sample = sample.unsqueeze(0).to(device)
    sample = torchvision.transforms.Resize((128, 128))(sample)
    sample = 1*(sample - torch.min(sample)) / (torch.max(sample) - torch.min(sample))
    sample = torch.exp(1j * 2*torch.pi * sample)
    sample = torch.nn.functional.pad(sample, (32, 32, 32, 32), mode='constant', value=0)







.. GENERATED FROM PYTHON SOURCE LINES 64-66

Optics forward model
-----------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 66-93

.. code-block:: Python

    from colibri.optics import CodedPhaseImaging
    from colibri.optics.functional import coded_phase_imaging_forward, coded_phase_imaging_backward


    img_size = sample.shape[1:]



    wave_length = 670e-9
    pixel_size = 1e-6
    sensor_distance = 50e-6
    approximation = "fresnel"



    acquisition_model = CodedPhaseImaging(
        input_shape=img_size,
        pixel_size=pixel_size,
        wavelength=wave_length,
        sensor_distance=sensor_distance,
        approximation=approximation,
        trainable=False,
    )


    y = acquisition_model(sample, type_calculation="forward", intensity=True)








.. GENERATED FROM PYTHON SOURCE LINES 94-96

Estimate phase 
-----------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 96-138

.. code-block:: Python


    from colibri.recovery import LFSI


    lfsi_algorithm = LFSI(
                        max_iters=15,
                        p=0.9,
                        k_size=5,
                        sigma=1.0,
                        train_filter=False,
                        dtype=torch.float32,
                        device=device,
    )

    x_hat = lfsi_algorithm(y, acquisition_model)


    sample = sample.detach().cpu().squeeze().angle()
    y = y.detach().cpu().squeeze()
    x_hat = x_hat.detach().cpu().squeeze().angle()

    normalize = lambda x: (x - torch.min(x)) / (torch.max(x) - torch.min(x))

    fig, axs = plt.subplots(1, 3, figsize=(15, 5))

    axs[0].set_title("Reference")
    axs[0].imshow(sample, cmap="gray")
    axs[0].set_xticks([])
    axs[0].set_yticks([])

    axs[1].set_title("Measurement")
    axs[1].imshow(y, cmap="gray")
    axs[1].set_xticks([])
    axs[1].set_yticks([])

    axs[2].set_title("Estimation")
    axs[2].imshow(x_hat, cmap="gray")
    axs[2].set_xticks([])
    axs[2].set_yticks([])

    plt.show()




.. image-sg:: /auto_examples/images/sphx_glr_demo_lfsi_001.png
   :alt: Reference, Measurement, Estimation
   :srcset: /auto_examples/images/sphx_glr_demo_lfsi_001.png
   :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 7.256 seconds)


.. _sphx_glr_download_auto_examples_demo_lfsi.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_lfsi.ipynb <demo_lfsi.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_lfsi.py <demo_lfsi.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: demo_lfsi.zip <demo_lfsi.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
